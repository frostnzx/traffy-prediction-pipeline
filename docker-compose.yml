version: "3.8"

services:
    # PostgreSQL database for Airflow metadata
    postgres:
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: always

    # Airflow webserver
    airflow-webserver:
        image: apache/airflow:2.7.0-python3.10
        depends_on:
            postgres:
                condition: service_healthy
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
            - AIRFLOW__CORE__FERNET_KEY=9Afntma2Zj4Wz2LNt95NubYZEdSH81QO4DOEZHIIYQQ=
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
            - _PIP_ADDITIONAL_REQUIREMENTS=pandas numpy scikit-learn joblib requests beautifulsoup4 gdown dill
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
            - ./data:/opt/airflow/data
            - ./models:/opt/airflow/models
        ports:
            - "8080:8080"
        command: >
            bash -c "airflow db migrate &&
                     airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true &&
                     airflow webserver"
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 60s
        restart: always

    # Airflow scheduler
    airflow-scheduler:
        image: apache/airflow:2.7.0-python3.10
        depends_on:
            airflow-webserver:
                condition: service_healthy
        environment:
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
            - AIRFLOW__CORE__FERNET_KEY=9Afntma2Zj4Wz2LNt95NubYZEdSH81QO4DOEZHIIYQQ=
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - _PIP_ADDITIONAL_REQUIREMENTS=pandas numpy scikit-learn joblib requests beautifulsoup4 gdown dill
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
            - ./data:/opt/airflow/data
            - ./models:/opt/airflow/models
        command: scheduler
        healthcheck:
            test:
                [
                    "CMD-SHELL",
                    'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"',
                ]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 120s
        restart: always

    # FastAPI prediction service
    api:
        build:
            context: .
            dockerfile: Dockerfile.api
        ports:
            - "8000:8000"
        volumes:
            - ./models:/app/models
            - ./api:/app/api
        environment:
            - PYTHONUNBUFFERED=1
        depends_on:
            airflow-scheduler:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8000/health"]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

    # Streamlit UI
    streamlit:
        build:
            context: .
            dockerfile: Dockerfile.streamlit
        ports:
            - "8501:8501"
        volumes:
            - ./models:/app/models
            - ./data:/app/data
            - ./app.py:/app/app.py
        environment:
            - PYTHONUNBUFFERED=1
        depends_on:
            airflow-scheduler:
                condition: service_healthy
        healthcheck:
            test:
                [
                    "CMD",
                    "curl",
                    "--fail",
                    "http://localhost:8501/_stcore/health",
                ]
            interval: 10s
            timeout: 5s
            retries: 5
        restart: always

volumes:
    postgres-db-volume:
